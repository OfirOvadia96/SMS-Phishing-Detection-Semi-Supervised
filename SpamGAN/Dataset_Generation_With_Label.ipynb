{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import supervised dataset\n",
    "\n",
    "import pandas as pd\n",
    "f_path = './dataset/SMSSpamCollection.csv'\n",
    "dataset = pd.read_csv(f_path,sep='\\t',names=[\"label\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for removing numbers and special characters\n",
    "\n",
    "def text_preprocess(sen): \n",
    "\n",
    "   sen = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "   sen = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sen)\n",
    "   sen = re.sub(r'\\s+', ' ', sen)\n",
    "\n",
    "   return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generate(sentence):\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)[len(sentence)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"message\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearing the numbers and special characters and generating new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** Takes about 90 hours ****\n",
    "\n",
    "X_messages = [] \n",
    "messages = list(X) \n",
    "for mes in messages: \n",
    "    sentence = text_preprocess(mes)\n",
    "    sen = text_generate(sentence)\n",
    "    X_messages.append(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the new synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame (X_messages, columns = ['message'])\n",
    "\n",
    "df.head(10)\n",
    "df[['message']].to_csv('SyntheticMessages.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the unsepervised synthetic dataset & basic supervised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "f_path = './dataset/SyntheticMessages.csv'\n",
    "synthetic_dataset = pd.read_csv(f_path)\n",
    "\n",
    "f_path = './dataset/SMSSpamCollection.csv'\n",
    "dataset = pd.read_csv(f_path,sep='\\t',names=[\"label\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing numbers and special characters\n",
    "\n",
    "def text_preprocess(sen): \n",
    "\n",
    "   sen = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "   sen = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sen)\n",
    "   sen = re.sub(r'\\s+', ' ', sen)\n",
    "\n",
    "   return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"message\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to list after preprocessing\n",
    "X_messages = [] \n",
    "messages = list(X) \n",
    "for mes in messages: \n",
    "    X_messages.append(text_preprocess(mes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Text to Numbers\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "tfidf_vec = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "X = tfidf_vec.fit_transform(X_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict synthetic dataset with all supervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained models\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./trained_models/NaiveBayes_model', 'rb') as f:\n",
    "    nb_model = pickle.load(f)\n",
    "\n",
    "with open('./trained_models/randomForest_model', 'rb') as f:\n",
    "    rf_model = pickle.load(f)\n",
    "\n",
    "with open('./trained_models/SVM_model', 'rb') as f:\n",
    "    svm_model = pickle.load(f)\n",
    "\n",
    "with open('./trained_models/KNeighbors_model', 'rb') as f:\n",
    "    kn_model = pickle.load(f)\n",
    "\n",
    "with open('./trained_models/DecisionTree_model', 'rb') as f:\n",
    "    dt_model = pickle.load(f)\n",
    "\n",
    "with open('./trained_models/Bagging_model', 'rb') as f:\n",
    "    b_model = pickle.load(f)\n",
    "\n",
    "with open('./trained_models/AdaBoost_model', 'rb') as f:\n",
    "    ab_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the messages of synthetic dataset to list\n",
    "df = list(synthetic_dataset['message'])\n",
    "\n",
    "# Convert the label of basic dataset to list\n",
    "label_expected = list(dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks with 7 algorithems if the SMS is spam or not\n",
    "\n",
    "def isSpam(sms):\n",
    "    ham = 0\n",
    "    spam = 0\n",
    "    if nb_model.predict(sms) == 'ham': ham += 1\n",
    "    else: spam += 1\n",
    "\n",
    "    if rf_model.predict(sms) == 'ham': ham += 1\n",
    "    else: spam += 1\n",
    "\n",
    "    if svm_model.predict(sms) == 'ham': ham += 1\n",
    "    else: spam += 1\n",
    "\n",
    "    if kn_model.predict(sms) == 'ham': ham += 1\n",
    "    else: spam += 1\n",
    "\n",
    "    if dt_model.predict(sms) == 'ham': ham += 1\n",
    "    else: spam += 1\n",
    "\n",
    "    if b_model.predict(sms) == 'ham': ham += 1\n",
    "    else: spam += 1\n",
    "\n",
    "    if ab_model.predict(sms) == 'ham': ham += 1\n",
    "    else: spam += 1\n",
    "\n",
    "    return spam > ham\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(string):\n",
    "    return string != string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification for messages we created with the GPT2\n",
    "dataset_with_label = []\n",
    "\n",
    "for i in range(0,len(df)-1):\n",
    "    if isNaN(df[i]):\n",
    "        continue\n",
    "    else:\n",
    "        sen = tfidf_vec.transform([df[i]]).toarray() # Fit the Data\n",
    "        if isSpam(sen):\n",
    "            row = [df[i], 'spam']\n",
    "            if label_expected[i] == 'spam':\n",
    "                dataset_with_label.append(row)\n",
    "        else: \n",
    "            row = [df[i], 'ham']\n",
    "            if label_expected[i] == 'ham':\n",
    "                dataset_with_label.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of synthetic with label to dataframe\n",
    "df_with_label = pd.DataFrame (dataset_with_label, columns = ['message','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new synthetic dataset with label\n",
    "df_with_label.to_csv('SyntheticMessages_WithLabel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised dataset\n",
    "\n",
    "import pandas as pd\n",
    "f_path = './dataset/SyntheticMessages_WithLabel.csv'\n",
    "synthetic_dataset = pd.read_csv(f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
